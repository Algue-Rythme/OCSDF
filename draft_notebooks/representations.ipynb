{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c35d7b2-554d-4488-800b-c3973bb96a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deel.lip.layers import (\n",
    "    SpectralDense,\n",
    "    SpectralConv2D,\n",
    "    ScaledL2NormPooling2D,\n",
    "    FrobeniusDense,\n",
    ")\n",
    "from deel.lip.model import Sequential\n",
    "from deel.lip.activations import GroupSort\n",
    "from deel.lip.losses import MulticlassHKR, MulticlassKR\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca0f1d51-ca17-4b7d-9753-fe3e2c0e83ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# standardize and reshape the data\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "mean = x_train.mean()\n",
    "std = x_train.std()\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "x_test = (x_test - mean) / std\n",
    "# one hot encode the labels\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b22520-87d5-42cd-941d-b1a0f09c0424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lbethune/anaconda3/envs/tf2/lib/python3.8/site-packages/deel/lip/model.py:54: UserWarning: Sequential model contains a layer wich is not a Lipschitz layer: flatten\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        # Lipschitz layers preserve the API of their superclass ( here Conv2D )\n",
    "        # an optional param is available: k_coef_lip which control the lipschitz\n",
    "        # constant of the layer\n",
    "        SpectralConv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3, 3),\n",
    "            activation=GroupSort(2),\n",
    "            use_bias=True,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "        ),\n",
    "        # usual pooling layer are implemented (avg, max...), but new layers are also available\n",
    "        ScaledL2NormPooling2D(pool_size=(2, 2), data_format=\"channels_last\"),\n",
    "        SpectralConv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3, 3),\n",
    "            activation=GroupSort(2),\n",
    "            use_bias=True,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "        ),\n",
    "        ScaledL2NormPooling2D(pool_size=(2, 2), data_format=\"channels_last\"),\n",
    "        # our layers are fully interoperable with existing keras layers\n",
    "        Flatten(),\n",
    "        SpectralDense(\n",
    "            32,\n",
    "            activation=GroupSort(2),\n",
    "            use_bias=True,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "        ),\n",
    "        SpectralDense(\n",
    "            32,\n",
    "            activation=GroupSort(2),\n",
    "            use_bias=True,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "        ),\n",
    "    ],\n",
    "    # similary model has a parameter to set the lipschitz constant\n",
    "    # to set automatically the constant of each layer\n",
    "    k_coef_lip=1.0,\n",
    "    name=\"hkr_model\",\n",
    ")\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086899e7-3feb-4f45-ba2a-dea244b4dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "buffer_size = 1024\n",
    "ds = tf.data.Dataset.from_tensor_slices(x_train).repeat().shuffle(buffer_size).batch(batch_size)\n",
    "num_steps = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d332de90-340a-46a4-b493-1312be5fa356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def train(model, opt, ds):\n",
    "    losses = []\n",
    "    with tqdm.tqdm(total=num_steps) as pb:\n",
    "        for step, batch in zip(range(num_steps), ds):\n",
    "            with tf.GradientTape() as tape:\n",
    "                z = model(batch, training=True)\n",
    "                z1 = tf.expand_dims(z, axis=0)\n",
    "                z2 = tf.expand_dims(z, axis=1)\n",
    "                norms = 0.5 * tf.reduce_sum((z1 - z2)**2, axis=-1)\n",
    "                loss = -tf.reduce_mean(norms) # maximize squared norm\n",
    "            gradient = tape.gradient(loss, model.trainable_variables)\n",
    "            opt.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "            losses.append(-loss)\n",
    "            pb.set_postfix(avg_loss=float(np.array(losses).mean()), loss=float(loss.numpy()))\n",
    "            pb.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a40a48c2-3908-4c65-b8b8-270a8bad85a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:27<00:00, 10.80it/s, avg_loss=283, loss=-300]\n"
     ]
    }
   ],
   "source": [
    "train(model, opt, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9778d0ce-51e3-4543-8de1-4ac295fc2ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S=0.341462105512619 2-inf=[('0.1654885709285736', '0.33081138134002686')] inf=[('0.3538089990615845', '0.8004620671272278')] 2=[(0.34146047, 0.3414621)]\n",
      "S=0.3499998450279236 2-inf=[('0.3499833941459656', '0.3499981760978699')] inf=[('2.3724565505981445', '3.3413302898406982')] 2=[(0.34996822, 0.34999985)]\n",
      "S=1.0 2-inf=[('0.9999678134918213', '0.9999791979789734')] inf=[('17.522907257080078', '19.696821212768555')] 2=[(0.9999474, 1.0)]\n",
      "S=0.9999974966049194 2-inf=[('0.9999940991401672', '0.9999967813491821')] inf=[('4.080472469329834', '4.843266010284424')] 2=[(0.9999814, 0.9999975)]\n",
      "2 tf.Tensor([[[4.4211484]]], shape=(1, 1, 1), dtype=float64)\n",
      "GradSeedNorm: tf.Tensor([[[[4.42114814]]]], shape=(1, 1, 1, 1), dtype=float64)\n",
      "Input:  Input 0.04421148144090759 0.04421148144090759\n",
      "0 <class 'deel.lip.layers.SpectralConv2D'> 1.02 1.02\n",
      "1 <class 'deel.lip.layers.ScaledL2NormPooling2D'> 0.98 0.95\n",
      "2 <class 'deel.lip.layers.SpectralConv2D'> 0.99 1.02\n",
      "3 <class 'deel.lip.layers.ScaledL2NormPooling2D'> 0.88 0.89\n",
      "4 <class 'tensorflow.python.keras.layers.core.Flatten'> 0.88 1.00\n",
      "5 <class 'deel.lip.layers.SpectralDense'> 0.84 0.96\n",
      "6 <class 'deel.lip.layers.SpectralDense'> 0.84 1.00\n",
      "Output:  Output 0.03725207335807163 0.03725207335807163\n",
      "3.6175479791067433 4.421148398604801\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAPh0lEQVR4nO3dfYxldX3H8ffHBeMDKksZ6BbcbkOxQogudtwSaRp8qitoF1pNpY0QS7o2kUZTbd2YtGJME0x9II0tdlHC2voQEiVSQOtmlVKrLg66LEvBQHRrwQ07iAaxjZXl2z/u2ToOM3vvzNyH+eH7ldzcc8793TmfnZ397JnfPefeVBWSpPY8adIBJEnLY4FLUqMscElqlAUuSY2ywCWpUUeNc2fHH398bdiwYZy7lKTm3XbbbQ9W1dT87WMt8A0bNjAzMzPOXUpS85L850LbnUKRpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGjfVKTEmPt2HbjRPZ7/7Lz5vIfjU8HoFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktSovgWe5ClJbk1ye5I7k7yr235ZkvuT7Olu544+riTpsEEu5Pkx8JKqeiTJ0cCXkny2e+wDVfXe0cWTJC2mb4FXVQGPdKtHd7caZShJUn8DzYEnWZNkD3AQ2FlVu7uHLk2yN8nVSdYu8tytSWaSzMzOzg4ntSRpsAKvqkNVtRE4GdiU5AzgSuAUYCNwAHjfIs/dXlXTVTU9NTU1lNCSpCWehVJVPwBuBjZX1QNdsT8GXAVsGn48SdJiBjkLZSrJsd3yU4GXAXcnWTdn2AXAvpEklCQtaJCzUNYBO5KsoVf411bVDUn+MclGei9o7gfeOLKUkqTHGeQslL3AmQtsf/1IEkmSBuKVmJLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjBvlU+qckuTXJ7UnuTPKubvtxSXYmuae7Xzv6uJKkwwY5Av8x8JKqej6wEdic5CxgG7Crqk4FdnXrkqQx6Vvg1fNIt3p0dytgC7Cj274DOH8UASVJCxtoDjzJmiR7gIPAzqraDZxYVQcAuvsTFnnu1iQzSWZmZ2eHFFuSNFCBV9WhqtoInAxsSnLGoDuoqu1VNV1V01NTU8uMKUmab0lnoVTVD4Cbgc3AA0nWAXT3B4cdTpK0uEHOQplKcmy3/FTgZcDdwPXAxd2wi4HPjCijJGkBRw0wZh2wI8kaeoV/bVXdkOQrwLVJLgG+A7x2hDklSfP0LfCq2gucucD27wEvHUUoSVJ/XokpSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1apA3s5Ke8DZsu3HSEaQl8whckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGDfKp9M9O8sUkdyW5M8mbu+2XJbk/yZ7udu7o40qSDhvkQp5HgbdW1deTPAO4LcnO7rEPVNV7RxdPkrSYQT6V/gBwoFv+YZK7gJNGHUySdGRLmgNPsgE4E9jdbbo0yd4kVydZu8hztiaZSTIzOzu7srSSpP83cIEnOQb4FPCWqnoYuBI4BdhI7wj9fQs9r6q2V9V0VU1PTU2tPLEkCRiwwJMcTa+8P1ZVnwaoqgeq6lBVPQZcBWwaXUxJ0nyDnIUS4CPAXVX1/jnb180ZdgGwb/jxJEmLGeQslLOB1wN3JNnTbXsHcGGSjUAB+4E3jiCfJGkRg5yF8iUgCzx00/DjSJIG5ZWYktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGDfKJPJKegDZsu3Fi+95/+XkT2/cTiUfgktQoC1ySGjXIp9I/O8kXk9yV5M4kb+62H5dkZ5J7uvu1o48rSTpskCPwR4G3VtVpwFnAm5KcDmwDdlXVqcCubl2SNCZ9C7yqDlTV17vlHwJ3AScBW4Ad3bAdwPkjyihJWsCS5sCTbADOBHYDJ1bVAeiVPHDCIs/ZmmQmyczs7OwK40qSDhu4wJMcA3wKeEtVPTzo86pqe1VNV9X01NTUcjJKkhYwUIEnOZpeeX+sqj7dbX4gybru8XXAwdFElCQtZJCzUAJ8BLirqt4/56HrgYu75YuBzww/niRpMYNciXk28HrgjiR7um3vAC4Hrk1yCfAd4LUjSShJWlDfAq+qLwFZ5OGXDjeOJGlQXokpSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNWqQT6W/OsnBJPvmbLssyf1J9nS3c0cbU5I03yBH4NcAmxfY/oGq2tjdbhpuLElSP30LvKpuAR4aQxZJ0hIctYLnXprkImAGeGtVfX+hQUm2AlsB1q9fv4Ld6efBhm03TjqC1Izlvoh5JXAKsBE4ALxvsYFVtb2qpqtqempqapm7kyTNt6wCr6oHqupQVT0GXAVsGm4sSVI/yyrwJOvmrF4A7FtsrCRpNPrOgSf5BHAOcHyS+4B3Auck2QgUsB944+giSpIW0rfAq+rCBTZ/ZARZJElL4JWYktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqP6FniSq5McTLJvzrbjkuxMck93v3a0MSVJ8w1yBH4NsHnetm3Arqo6FdjVrUuSxqhvgVfVLcBD8zZvAXZ0yzuA84cbS5LUz3LnwE+sqgMA3f0Jiw1MsjXJTJKZ2dnZZe5OkjTfyF/ErKrtVTVdVdNTU1Oj3p0k/dxYboE/kGQdQHd/cHiRJEmDWG6BXw9c3C1fDHxmOHEkSYMa5DTCTwBfAX4tyX1JLgEuB16e5B7g5d26JGmMjuo3oKouXOShlw45i+bZsO3Giex3/+XnTWS/kpbGKzElqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY3q+4EO+vkzqQ+SkEZtkj/bo/igFI/AJalRFrgkNWpFUyhJ9gM/BA4Bj1bV9DBCSZL6G8Yc+Iur6sEhfB1J0hI4hSJJjVrpEXgBn09SwD9U1fb5A5JsBbYCrF+/foW7k/RE4JlOw7HSI/Czq+oFwCuBNyX5rfkDqmp7VU1X1fTU1NQKdydJOmxFBV5V3+3uDwLXAZuGEUqS1N+yCzzJ05M84/Ay8NvAvmEFkyQd2UrmwE8Erkty+Ot8vKo+N5RUkqS+ll3gVfUt4PlDzCJJWgJPI5SkRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNWqln0o/Nn6KtST9LI/AJalRFrgkNWpFBZ5kc5JvJrk3ybZhhZIk9bfsAk+yBvg74JXA6cCFSU4fVjBJ0pGt5Ah8E3BvVX2rqv4X+CSwZTixJEn9rOQslJOA/5qzfh/wG/MHJdkKbO1WH0nyzT5f93jgwRXkGoXVmAnMtVTmWhpzLc0Rc+U9K/rav7zQxpUUeBbYVo/bULUd2D7wF01mqmp6BbmGbjVmAnMtlbmWxlxLM4lcK5lCuQ949pz1k4HvriyOJGlQKynwrwGnJvmVJE8GXgdcP5xYkqR+lj2FUlWPJrkU+BdgDXB1Vd05hEwDT7eM0WrMBOZaKnMtjbmWZuy5UvW4aWtJUgO8ElOSGmWBS1Kjxl7gSZ6S5NYktye5M8m7Fhl3TpI93Zh/XQ25kvx5l2lPkn1JDiU5bhXkelaSf54z5g2jzLSEXGuTXJdkbzf2jFHn6va7Jsk3ktywwGNJ8rfd2z/sTfKCcWQaINdzk3wlyY+TvG1cmQbI9Yfd92lvki8nef4qybWly7QnyUyS31wNueaMeWHXD68ZaZiqGuuN3vnjx3TLRwO7gbPmjTkW+A9gfbd+wmrINW/8q4EvrIZcwDuA93TLU8BDwJNXQa6/Ad7ZLT8X2DWmn7E/Az4O3LDAY+cCn+3ynwXsHkemAXKdALwQ+GvgbePKNECuFwFru+VXrqLv1zH89DW85wF3r4Zc3eNrgC8ANwGvGWWWsR+BV88j3erR3W3+K6l/AHy6qr7TPefgKsk114XAJ1ZJrgKekST0frAfAh5dBblOB3Z14+8GNiQ5cZS5kpwMnAd8eJEhW4CPdvm/ChybZN0oMw2Sq6oOVtXXgJ+MOssSc325qr7frX6V3vUeqyHXI9W1JfB0jvxvdWy5On8KfAoYeW9NZA68+xVkD70/4M6q2j1vyHOAtUluTnJbkotWSa7D454GbKb3l7Qacn0QOI3ehVR3AG+uqsdWQa7bgd/txm6idznwqAvgCuAvgMX+/Au9BcRJI84E/XNNyhUMnusSer+9jMMV9MmV5IIkdwM3An+0GnIlOQm4APjQOMJMpMCr6lBVbaT3j3nTAnOjRwG/Tu9/ulcAf5nkOasg12GvBv69qh4adaYBc70C2AP8ErAR+GCSZ66CXJfT+494D72jkm8wwt8MkrwKOFhVtx1p2ALbRnr0NmCusVtKriQvplfgb18tuarquqp6LnA+8O5VkusK4O1VdWjUeWDCZ6FU1Q+Am+kdzc51H/C5qvpRVT0I3AKM7cWTI+Q67HWMYfpkviPkegO9KaeqqnuBb9Obc55orqp6uKre0JX8RfTm5789wihnA7+TZD+9d8d8SZJ/mjdmEm8BMUiuSRgoV5Ln0Zsy2FJV31stuQ6rqluAU5IcvwpyTQOf7Ma8Bvj7JOePLNGoJ/wXmOCfAo7tlp8K/BvwqnljTqM3d3oU8DRgH3DGpHN1jz2L3hzz01fR9+tK4LJu+UTgfuD4VZDrWLoXU4E/pjf3PK6fs3NY+MWv8/jZFzFvHVemI+Wa8/hljPlFzD7fr/XAvcCLxp2pT65f5acvYr6g+5nPpHPNG3MNI34RcxIfarwO2JHeB0I8Cbi2qm5I8icAVfWhqroryeeAvfTmmj5cVfsmnasbdwHw+ar60YjzLCXXu4FrktxBr5jeXr3fXCad6zTgo0kO0Tur6JIRZ1rQvEw30TsT5V7gv+n99jIRc3Ml+UVgBngm8FiStwCnV9XDk8wF/BXwC/SOJAEerQm9E+C8XL8HXJTkJ8D/AL9fXWtOONd49z2hP7MkaYW8ElOSGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEb9H9juj2F7qp2XAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import check_grads\n",
    "\n",
    "batch = next(ds.__iter__())\n",
    "check_grads(model, batch, plot_wandb=False, condense=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "204154f7-bc31-4bad-99b3-7f06401f67c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "num_classes = 10\n",
    "classifier = Dense(num_classes)\n",
    "opt_classifier = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "_ = classifier(model(batch, training=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f99eb0fc-830d-4637-bc82-ca66a4a1a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_classifier = tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(buffer_size).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1279ef5-c289-4221-a7b3-98331a4457f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:26<00:00, 11.23it/s, acc=81.2, avg_acc=79.2, avg_loss=0.675, loss=0.564]\n"
     ]
    }
   ],
   "source": [
    "def train_classifier(classifier, model, opt, ds):\n",
    "    losses = []\n",
    "    accs = []\n",
    "    with tqdm.tqdm(total=num_steps) as pb:\n",
    "        for step, batch in zip(range(num_steps), ds):\n",
    "            images, labels = batch\n",
    "            with tf.GradientTape() as tape:\n",
    "                z = model(images, training=True)\n",
    "                logits = classifier(z, training=True)\n",
    "                losses_elementwise = tf.nn.softmax_cross_entropy_with_logits(labels, logits)\n",
    "                loss = tf.reduce_mean(losses_elementwise)\n",
    "            preds = tf.argmax(logits, axis=-1)\n",
    "            acc = tf.reduce_mean(tf.cast(preds == tf.argmax(labels, axis=-1), dtype=tf.float32)) * 100\n",
    "            gradient = tape.gradient(loss, classifier.trainable_variables)\n",
    "            opt.apply_gradients(zip(gradient, classifier.trainable_variables))\n",
    "            losses.append(loss); accs.append(acc)\n",
    "            pb.set_postfix(avg_loss=float(np.array(losses).mean()), loss=float(loss.numpy()),\n",
    "                           avg_acc=float(np.array(accs).mean()), acc=float(acc.numpy()))\n",
    "            pb.update()\n",
    "train_classifier(classifier, model, opt_classifier, ds_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b9772d0c-8151-491f-b549-692edcf1560c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:09, 110.17it/s, acc=92.2, avg_acc=88.4, avg_loss=0.394, loss=0.278]                        \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "num_classes = 10\n",
    "classifier = Dense(num_classes)\n",
    "opt_classifier = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "_ = classifier(tf.reshape(batch, shape=(len(batch), 784)))\n",
    "def train_classifier(classifier, model, opt, ds):\n",
    "    losses = []\n",
    "    accs = []\n",
    "    with tqdm.tqdm(total=num_steps) as pb:\n",
    "        for step, batch in zip(range(1000), ds):\n",
    "            images, labels = batch\n",
    "            with tf.GradientTape() as tape:\n",
    "                z = tf.reshape(images, shape=(len(images), 784))\n",
    "                logits = classifier(z, training=True)\n",
    "                losses_elementwise = tf.nn.softmax_cross_entropy_with_logits(labels, logits)\n",
    "                loss = tf.reduce_mean(losses_elementwise)\n",
    "            preds = tf.argmax(logits, axis=-1)\n",
    "            acc = tf.reduce_mean(tf.cast(preds == tf.argmax(labels, axis=-1), dtype=tf.float32)) * 100\n",
    "            gradient = tape.gradient(loss, classifier.trainable_variables)\n",
    "            opt.apply_gradients(zip(gradient, classifier.trainable_variables))\n",
    "            losses.append(loss); accs.append(acc)\n",
    "            pb.set_postfix(avg_loss=float(np.array(losses).mean()), loss=float(loss.numpy()),\n",
    "                           avg_acc=float(np.array(accs).mean()), acc=float(acc.numpy()))\n",
    "            pb.update()\n",
    "train_classifier(classifier, model, opt_classifier, ds_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c770bf-274a-447a-a924-01c31930d17e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "92c440f6d82e79b41f51da7601df579ae39fc115954f77d4b816ac1ce1ea7f75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
