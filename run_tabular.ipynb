{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28162bc-170e-4c6a-84af-e4974ce75a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs available:\", physical_devices)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b53ff9-7834-443e-8330-927c0008a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ocml.datasets import build_ds_from_numpy, tfds_from_sampler\n",
    "from ocml.evaluate import check_LLC, log_metrics, evaluate_tabular\n",
    "from ocml.models import conventional_dense, spectral_dense\n",
    "from ocml.plot import plot_preds_ood\n",
    "from ocml.priors import uniform_tabular\n",
    "from ocml.train import train, SH_KR, BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd3930-12e7-48b4-b7f8-6a30c7b71957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "def get_config(debug=False):\n",
    "  dataset_name = os.environ.get(\"DATASET_NAME\", \"thyroid\")\n",
    "  adoc = os.environ.get(\"ADOC\", \"ad\")  # or 'ad'\n",
    "  config = SimpleNamespace(\n",
    "    dataset_name = dataset_name,\n",
    "    batch_size = 128,  # should be not too small to ensure diversity.\n",
    "    domain = [-5, 5.],  # domain on which to sample points.\n",
    "    use_sampler_for_train_test = True,\n",
    "    scaling = False,\n",
    "    maxiter = 4,  # very important on high dimensional dataset.\n",
    "    margin = 0.05,  # very important !\n",
    "    lbda = 100.,  # important but not as much as `margin`. Must be high for best results.\n",
    "    k_coef_lip = 1.,  # no reason to change this.\n",
    "    spectral_dense = True,  # Mandatory for orthogonal networks. \n",
    "    deterministic = False,  # Better with random learning rates.\n",
    "    overshoot_boundary = False,\n",
    "    conventional = False,  # Conventional training (i.e without hKR and Lipschitz constraint) for sanity check.\n",
    "    widths = [512, 512, 512, 512],\n",
    "    warmup_epochs=5,\n",
    "    epochs_per_plot=20,\n",
    "    adoc = adoc\n",
    "  )\n",
    "  return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5229cd0-f6c6-4b19-bbd2-f6f47aa52718",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = \"SANDBOX\" in os.environ\n",
    "config = get_config(debug)\n",
    "train_kwargs = {\n",
    "  'domain': config.domain,\n",
    "  'deterministic': config.deterministic,\n",
    "  'overshoot_boundary': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825e96b-a921-47ee-9ffc-4dcb2b07ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "print(\"PLOTLY_RENDERER:\", pio.renderers.default)\n",
    "try:\n",
    "  import os\n",
    "  os.environ['WANDB_NOTEBOOK_NAME'] = 'run_tabular.ipynb'\n",
    "  import wandb\n",
    "  wandb.login()\n",
    "  wandb_available = True\n",
    "except ModuleNotFoundError as e:\n",
    "  print(e)\n",
    "  print(\"Wandb logs will be removed.\")\n",
    "  wandb_available = False\n",
    "plot_wandb = wandb_available and not debug  # Set to False to de-activate Wandb.\n",
    "if plot_wandb:  \n",
    "  import wandb\n",
    "  group = os.environ.get(\"WANDB_GROUP\", \"sandbox_tabular\")\n",
    "  wandb.init(project=\"ocml_tabular\", group=group, config=config.__dict__)\n",
    "else:\n",
    "  try:\n",
    "    wandb.finish()\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "train_kwargs['log_metrics_fn'] = partial(log_metrics, plot_wandb=plot_wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d76d182-36ed-41e8-a89e-47c1f801225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "datasets = {\n",
    "  'thyroid': {\n",
    "    'save_path': '/data/datasets/tabular/thyroid.mat',\n",
    "    'origin': 'https://www.dropbox.com/s/bih0e15a0fukftb/thyroid.mat?dl=1'\n",
    "  },\n",
    "  'mammography': {\n",
    "    'save_path': '/data/datasets/tabular/mammography.mat',\n",
    "    'origin': 'https://www.dropbox.com/s/tq2v4hhwyv17hlk/mammography.mat?dl=1'\n",
    "  },\n",
    "  'arrhythmia': {\n",
    "    'save_path': '/data/datasets/tabular/arrhythmia/arrhythmia.mat',\n",
    "    'origin': 'https://www.dropbox.com/s/lmlwuspn1sey48r/arrhythmia.mat?dl=1'\n",
    "  },\n",
    "  'arrhythmia_uci': {\n",
    "    'save_path': '/data/datasets/tabular/arrhythmia/arrhythmia.data',\n",
    "    'origin': 'https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.data'\n",
    "  }\n",
    "}\n",
    "\n",
    "try:\n",
    "  save_path = datasets[config.dataset_name]['save_path']\n",
    "  origin = datasets[config.dataset_name]['origin']\n",
    "  dataset_path = get_file(save_path, origin=origin)\n",
    "  print(\"Dataset used:\", dataset_path)\n",
    "except:\n",
    "  print('Error downloading')\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ef4a0-283a-4c3b-8077-b111388b8c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat  # this is the SciPy module that loads mat-files\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date, time\n",
    "import pandas as pd\n",
    "\n",
    "def load_matfile(file_path):\n",
    "    mat = loadmat(file_path)\n",
    "    mat = {k:v for k, v in mat.items() if k[0] != '_'}\n",
    "    df = pd.DataFrame(np.concatenate([mat['X'],mat['y']], axis=1))\n",
    "    return df\n",
    "  \n",
    "if config.dataset_name == 'arrhythmia_uci':\n",
    "  \"Custom processing for UCI original source. Shall not be used.\"\n",
    "  from sklearn.experimental import enable_iterative_imputer\n",
    "  from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "  df = pd.read_csv(dataset_path, header=None, na_values='?')\n",
    "  imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "  df = pd.DataFrame(imputer.fit_transform(df))\n",
    "  dtypes = ['float64']*16 + (['int64']*6 + ['float64']*6)*12 + (['float64']*10)*12\n",
    "  dtypes = {idx: dtype for idx, dtype in zip(df.columns, dtypes)}\n",
    "  df = df.astype(dtypes)\n",
    "  discrete_cols = [idx for idx in dtypes if dtypes[idx] == 'int64']\n",
    "  print('Categorical :', discrete_cols)\n",
    "else:\n",
    "  df = load_matfile(dataset_path)\n",
    "  last_idx = int(df.columns[-1])\n",
    "  df.rename({last_idx:'label'},axis=1,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07479578-0f5d-473c-b3f5-91f8c058861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normality = df[df['label'] == 0.].drop('label', axis=1)\n",
    "anomalies = df[df['label'] == 1.].drop('label', axis=1).to_numpy()\n",
    "print(f\"Normality={len(normality)} Anomaly={len(anomalies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0978ad32-10c7-4cef-8eee-21e130403a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e26bf-aabe-4cbb-9abf-d0b345ac9bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "print('Building tree... ')\n",
    "kdt = KDTree(normality, leaf_size=30, metric='euclidean')\n",
    "print('Built ! Queries on going... ')\n",
    "dists, indexes = kdt.query(anomalies, k=20, return_distance=True)\n",
    "print('Distances of each anomaly to 20 nearest normal points')\n",
    "pd.DataFrame(dists).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5dde1-ca3e-4f79-aee4-b3b3de347f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocml.priors import TabularSampler\n",
    "sampler = TabularSampler(bounds=None)\n",
    "continuous_policy = 'zscore'\n",
    "if config.adoc == 'ad':\n",
    "  dt = sampler.fit_transform(df, df, continuous_policy=continuous_policy)\n",
    "elif config.adoc == 'oc':\n",
    "  dt = sampler.fit_transform(df, normality, continuous_policy=continuous_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c2264d-c041-4367-bf5a-b5538dfd277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt[dt['label'] == 0.].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85859f-af1f-4011-a351-f52ec978d94b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dt[dt['label'] == 1.].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b86ebd-11c0-4bda-b75d-1adf114bcc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if config.use_sampler_for_train_test:\n",
    "  print(\"Use dataset rescaled properly with Sampler object.\")\n",
    "  normality = dt[dt['label'] == 0.].drop('label', axis=1)\n",
    "  anomalies = dt[dt['label'] == 1.].drop('label', axis=1).to_numpy()\n",
    "  print(f\"Normality={len(normality)} Anomaly={len(anomalies)}\")\n",
    "\n",
    "if config.adoc == 'ad':\n",
    "  print(\"Running as Anomaly Detection.\")\n",
    "  x_train = np.concatenate([normality.to_numpy(), anomalies], axis=0)  # train on everything including anomalies.\n",
    "  x_test = normality.to_numpy()  # anomalies treated separatly to avoid biases.\n",
    "elif config.adoc == 'oc':\n",
    "  train_sizes = {\n",
    "    # Default sizes from DAGMM/HRN protocols.\n",
    "    'thyroid': 1839,\n",
    "    'arrhythmia': 193\n",
    "  }\n",
    "  train_size = train_sizes.get(config.dataset_name, round(len(normality) * 0.5))\n",
    "  x_train, x_test = train_test_split(normality.to_numpy(), train_size=train_size, shuffle=True)\n",
    "  print(f\"Train Size={len(x_train)} Test Size={len(x_test)+len(anomalies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c72cc-8f54-4702-bfab-cc6991ba6d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_train).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7319ae86-315e-444d-975e-9d74e893c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(anomalies).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e4b514-8e5d-4074-b57a-fe045f77fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_length = math.ceil(len(x_train) / config.batch_size)\n",
    "epoch_length = max(epoch_length, 15)\n",
    "print(f\"Epoch Length={epoch_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d57ce5-2cc9-4f70-991a-70dd1821d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as preprocessing\n",
    "\n",
    "if config.scaling:\n",
    "  # Should be useless when using Sampler object because it already rescale data.\n",
    "  scaler = preprocessing.StandardScaler()\n",
    "  x_train = scaler.fit_transform(x_train)\n",
    "  print(f\"Scaler: mean={scaler.mean_[:10]} std={scaler.scale_[:10]} min_scale={np.min(np.array(scaler.scale_))}\")\n",
    "  x_test = scaler.transform(x_test)\n",
    "  normality = scaler.transform(normality)\n",
    "  anomalies = scaler.transform(anomalies)\n",
    "  print('x_train max norm:', pd.DataFrame(x_train).describe().iloc[1].max())\n",
    "  print('x_test max norm:', pd.DataFrame(x_test).describe().iloc[1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955ed4b4-c1cd-4a7d-be09-6f45c98e7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, g_sampler in enumerate(sampler.samplers[:10]):\n",
    "  # Data should be centered. True STD depends on the scale of the whole fit test (that may include anomalies).\n",
    "  scale = g_sampler.std*g_sampler.threshold\n",
    "  mean = g_sampler.mean\n",
    "  print(f'[{i}] {mean:.5f}±{scale:.5f}, sample in [{mean-3*scale:.5f},{scale+3*scale:.5f}] at 99.5% confidence interval.')\n",
    "scales = np.array([g_sampler.std*g_sampler.threshold for g_sampler in sampler.samplers])\n",
    "min_scale_idx = np.argmin(scales)\n",
    "print(f'MinScale={scales[min_scale_idx]} at center {sampler.samplers[min_scale_idx].mean} with idx={min_scale_idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc4115-e744-46d6-ad72-103748bb5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = pd.DataFrame(sampler.sample(5))\n",
    "adv.columns = dt.columns[:-1]\n",
    "adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d454a-8ec6-4b24-85df-d3b18cf021cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "print('Building tree... ')\n",
    "kdt = KDTree(normality, leaf_size=30, metric='euclidean')\n",
    "print('Built ! Queries on going... ')\n",
    "dists, indexes = kdt.query(adv.to_numpy(), k=20, return_distance=True)\n",
    "pd.DataFrame(dists).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a7fda-19f6-47de-8334-b700cd9107e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create positive examples dataset.\n",
    "p_dataset = build_ds_from_numpy(x_train, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b70da1-8a9b-4cb8-a4d9-e3fa01022848",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = adv.shape[1]\n",
    "\n",
    "# Train model.\n",
    "if config.conventional:\n",
    "  model = conventional_dense(widths=config.widths, input_shape=(input_size,))\n",
    "else:\n",
    "  model = spectral_dense(widths=config.widths, input_shape=(input_size,),\n",
    "                         k_coef_lip=config.k_coef_lip)\n",
    "\n",
    "if config.conventional:\n",
    "  loss_fn = BCE()\n",
    "else:\n",
    "  loss_fn = SH_KR(config.margin, config.lbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b911ab30-ec8c-4f3c-bf47-c6b2366590a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer class.\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.0005)\n",
    "\n",
    "# Initialize the network.\n",
    "gen = tf.random.Generator.from_seed(4321)  # reproducible sampling.\n",
    "p_batch = next(iter(p_dataset))\n",
    "_ = model(p_batch, training=True)  # dummy forward to trigger initialization.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ebba1-1465-4d14-b332-9a512e810adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial distribution.\n",
    "def sampler_fn(gen, batch_size, input_shape):\n",
    "  del gen  # unused.\n",
    "  del input_shape  # unused.\n",
    "  return sampler.sample(batch_size)\n",
    "\n",
    "q_dataset = tfds_from_sampler(sampler_fn, gen, config.batch_size, p_batch.shape[1:])\n",
    "Q0 = next(iter(q_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1087c2-489d-40df-8055-d1b302333170",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = config.warmup_epochs\n",
    "for epoch in range(num_epochs):\n",
    "  train(model, opt, loss_fn, gen, p_dataset, q_dataset, epoch_length, maxiter=0, **train_kwargs)\n",
    "  T = evaluate_tabular(epoch, model, x_test, anomalies, plot_wandb=plot_wandb)\n",
    "plot_preds_ood(epoch, model, tf.constant(x_train), tf.constant(x_test), tf.constant(anomalies), plot_histogram=True, plot_wandb=False, T=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8384b0c2-dff0-4fba-8e4f-e64ccd89b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(config.epochs_per_plot):\n",
    "  train(model, opt, loss_fn, gen, p_dataset, q_dataset, epoch_length, maxiter=config.maxiter, **train_kwargs)\n",
    "  T = evaluate_tabular(epoch, model, x_test, anomalies, plot_wandb=plot_wandb)\n",
    "plot_preds_ood(epoch, model, tf.constant(x_train), tf.constant(x_test), tf.constant(anomalies), plot_histogram=True, plot_wandb=plot_wandb, T=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f5677d-9447-40a9-b6cf-1c21b405b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(config.epochs_per_plot):\n",
    "  train(model, opt, loss_fn, gen, p_dataset, q_dataset, epoch_length, maxiter=config.maxiter, **train_kwargs)\n",
    "  T = evaluate_tabular(epoch, model, x_test, anomalies, plot_wandb=plot_wandb)\n",
    "plot_preds_ood(epoch, model, tf.constant(x_train), tf.constant(x_test), tf.constant(anomalies), plot_histogram=True, plot_wandb=plot_wandb, T=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9417869d-0014-4be0-bc1e-6c53236035ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_wandb:\n",
    "  wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75cb657-4d73-4de7-8ada-2fe68f192840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d81943c-0a63-4d62-90f5-59cb01c487c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d555d-5a82-4499-8be9-75efba760865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e4d680-3741-4eac-8d59-687f9fe1bf96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d55e15-a474-40cb-9d72-7833c343a6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
