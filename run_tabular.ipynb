{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28162bc-170e-4c6a-84af-e4974ce75a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs available:\", physical_devices)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b53ff9-7834-443e-8330-927c0008a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ocml.datasets import build_ds_from_numpy, tfds_from_sampler\n",
    "from ocml.evaluate import check_LLC, log_metrics\n",
    "from ocml.models import conventional_dense, spectral_dense\n",
    "from ocml.plot import plot_preds_ood\n",
    "from ocml.priors import uniform_tabular\n",
    "from ocml.train import train, SH_KR, BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd3930-12e7-48b4-b7f8-6a30c7b71957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "def get_config(debug=False):\n",
    "  config = SimpleNamespace(\n",
    "    batch_size = 128,  # should be not too small to ensure diversity.\n",
    "    domain = [-5, 5.],  # domain on which to sample points.\n",
    "    scaling = False,\n",
    "    maxiter = 4,  # very important on high dimensional dataset.\n",
    "    margin = 0.05,  # very important !\n",
    "    lbda = 100.,  # important but not as much as `margin`. Must be high for best results.\n",
    "    k_coef_lip = 1.,  # no reason to change this.\n",
    "    spectral_dense = True,  # Mandatory for orthogonal networks. \n",
    "    deterministic = False,  # Better with random learning rates.\n",
    "    overshoot_boundary = False,\n",
    "    conventional = False,  # Conventional training (i.e without hKR and Lipschitz constraint) for sanity check.\n",
    "    widths = [512, 512, 512, 512],\n",
    "    warmup_epochs=5,\n",
    "    epochs_per_plot=20,\n",
    "  )\n",
    "  return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5229cd0-f6c6-4b19-bbd2-f6f47aa52718",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = \"SANDBOX\" in os.environ\n",
    "config = get_config(debug)\n",
    "train_kwargs = {\n",
    "  'domain': config.domain,\n",
    "  'deterministic': config.deterministic,\n",
    "  'overshoot_boundary': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825e96b-a921-47ee-9ffc-4dcb2b07ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "print(\"PLOTLY_RENDERER:\", pio.renderers.default)\n",
    "try:\n",
    "  import os\n",
    "  os.environ['WANDB_NOTEBOOK_NAME'] = 'run_tabular.ipynb'\n",
    "  import wandb\n",
    "  wandb.login()\n",
    "  wandb_available = True\n",
    "except ModuleNotFoundError as e:\n",
    "  print(e)\n",
    "  print(\"Wandb logs will be removed.\")\n",
    "  wandb_available = False\n",
    "plot_wandb = wandb_available and not debug  # Set to False to de-activate Wandb.\n",
    "if plot_wandb:  \n",
    "  import wandb\n",
    "  group = os.environ.get(\"WANDB_GROUP\", \"sandbox_tabular\")\n",
    "  wandb.init(project=\"oneclass\", group=group, config=config.__dict__)\n",
    "else:\n",
    "  try:\n",
    "    wandb.finish()\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "train_kwargs['log_metrics_fn'] = partial(log_metrics, plot_wandb=plot_wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e96a80-97bd-445d-b5d9-b70e4828eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.\n",
    "if config.conventional:\n",
    "  model = conventional_dense(widths=config.widths, input_shape=(6,))\n",
    "else:\n",
    "  model = spectral_dense(widths=config.widths, input_shape=(6,),\n",
    "                         k_coef_lip=config.k_coef_lip)\n",
    "\n",
    "if config.conventional:\n",
    "  loss_fn = BCE()\n",
    "else:\n",
    "  loss_fn = SH_KR(config.margin, config.lbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d76d182-36ed-41e8-a89e-47c1f801225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "try:\n",
    "  path = '/data/datasets/tabular/thyroid.mat'\n",
    "  thyroid_path = get_file(path, origin='https://www.dropbox.com/s/bih0e15a0fukftb/thyroid.mat?dl=1')\n",
    "  print(thyroid_path)\n",
    "except:\n",
    "  print('Error downloading')\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ef4a0-283a-4c3b-8077-b111388b8c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat  # this is the SciPy module that loads mat-files\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date, time\n",
    "import pandas as pd\n",
    "\n",
    "def load_matfile(file_path):\n",
    "    mat = loadmat(file_path)\n",
    "    mat = {k:v for k, v in mat.items() if k[0] != '_'}\n",
    "    df = pd.DataFrame(np.concatenate([mat['X'],mat['y']], axis=1))\n",
    "    df.rename({6:'label'},axis=1,inplace=True)\n",
    "    return df\n",
    "  \n",
    "df = load_matfile(thyroid_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08ea96-35fe-456c-adee-100c77387d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07479578-0f5d-473c-b3f5-91f8c058861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normality = df[df['label'] == 0.].drop('label', axis=1)\n",
    "anomalies = df[df['label'] == 1.].drop('label', axis=1).to_numpy()\n",
    "print(f\"Normality={len(normality)} Anomaly={len(anomalies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d64723-16c3-4683-8026-956951d0ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test = train_test_split(normality.to_numpy(), train_size=1839)\n",
    "print(f\"Train Size={len(x_train)} Test Size={len(x_test)+len(anomalies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd930e8-4815-439b-97eb-19deff729369",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_length = math.ceil(len(x_train) / config.batch_size)\n",
    "print(f\"Epoch Length={epoch_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d57ce5-2cc9-4f70-991a-70dd1821d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as preprocessing\n",
    "\n",
    "if config.scaling:\n",
    "  scaler = preprocessing.StandardScaler()\n",
    "  x_train = scaler.fit_transform(x_train)\n",
    "  x_test = scaler.transform(x_test)\n",
    "  normality = scaler.transform(normality)\n",
    "  anomalies = scaler.transform(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e26bf-aabe-4cbb-9abf-d0b345ac9bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "print('Building tree... ')\n",
    "kdt = KDTree(normality, leaf_size=30, metric='euclidean')\n",
    "print('Built ! Queries on going... ')\n",
    "dists, indexes = kdt.query(anomalies, k=20, return_distance=True)\n",
    "print('Distances of each anomaly to 20 nearest normal points')\n",
    "pd.DataFrame(dists).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228bfd56-a2ee-4aab-a2c4-3069b5b56550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorical:\n",
    "  def __init__(self, num_classes):\n",
    "    self.num_classes = max(num_classes, 2)\n",
    "  def scale_bounds(self, bounds):\n",
    "    pass\n",
    "  def is_outlier(x):\n",
    "    return np.full(x.shape[0], False, dtype=bool)\n",
    "  def encode(x):\n",
    "    if self.num_classes == 2:\n",
    "      return x.copy()\n",
    "    one_hot = np.eye(self.num_classes)[x]\n",
    "    return one_hot\n",
    "  def sample(self, batch_size):\n",
    "    if self.num_classes == 2:\n",
    "      return np.random.randint(2, size=batch_size)[:,np.newaxis]\n",
    "    classes = np.random.randint(self.num_classes, size=batch_size)\n",
    "    one_hot = np.eye(self.num_classes)[classes]\n",
    "    return one_hot\n",
    "\n",
    "class Gaussian:\n",
    "  def __init__(self, mean, std, bounds):\n",
    "    self.mean = mean\n",
    "    self.std = std\n",
    "    self.threshold = bounds\n",
    "  def scale_bounds(self, bounds):\n",
    "    self.threshold *= bounds\n",
    "  def is_outlier(x):\n",
    "    return np.abs(x) > self.threshold * self.std\n",
    "  def encode(x):\n",
    "    return (x - mean) / std\n",
    "  def sample(self, batch_size):\n",
    "    emp_std = self.threshold * self.std\n",
    "    return np.random.normal(self.mean, emp_std, (batch_size,1))\n",
    "\n",
    "class LogUniform:\n",
    "  def __init__(self, min_v, max_v, shift):\n",
    "    self.min_v = min_v\n",
    "    self.max_v = max_v\n",
    "    self.shift = shift\n",
    "    self.bounds = 1.\n",
    "  def scale_bounds(self, bounds):\n",
    "    self.bounds *= bounds\n",
    "  def is_outlier(x):\n",
    "    return np.logical_or(x < self.min_v, x > self.max_v)\n",
    "  def encode(x):\n",
    "    return np.log(x + self.shift)\n",
    "  def sample(self, batch_size):\n",
    "    min_v, max_v = self.min_v * self.bounds, self.max_v * self.bounds\n",
    "    return np.random.uniform(min_v, max_v, (batch_size,1))  # uniform in log space\n",
    "\n",
    "class Sampler:\n",
    "    def __init__(self, bounds, samplers=None):\n",
    "      self.samplers = [] if samplers is None else samplers\n",
    "      self.bounds = bounds\n",
    "      self.shift = 0.1\n",
    "    def scale_bounds(self, bounds):\n",
    "      self.bounds *= bounds\n",
    "      for sampler in self.samplers:\n",
    "          sampler.scale_bounds(bounds)\n",
    "    def add(self, sampler):\n",
    "      self.samplers.append(sampler)\n",
    "    def check_integrity(self, batch_size, batch_size_ref):\n",
    "      assert self.sample(batch_size).shape == (batch_size,) + batch_size_ref\n",
    "    def encode_numeric_zscore(self, df, df_source, df_train, name, mean=None, sd=None):\n",
    "      if mean is None:\n",
    "          mean = df_train[name].mean()\n",
    "      if sd is None:\n",
    "          sd = df_train[name].std()\n",
    "      if sd == 0:\n",
    "          sd = 1\n",
    "      df[name] = (df_source[name] - mean) / sd\n",
    "      bounds = max(df[name].max(), -df[name].min()) / 2\n",
    "      sampler = Gaussian(0., 1., bounds)\n",
    "      self.add(sampler)\n",
    "    def encode_logscale(self, df, df_source, name):\n",
    "      df[name] = np.log(df_source[name] + self.shift)\n",
    "      min_v = df[name].min()\n",
    "      max_v = df[name].max()\n",
    "      sampler = LogUniform(min_v, max_v, self.shift)\n",
    "      self.add(sampler)\n",
    "    def encode_robust_zscore(self, df, df_source, df_train, name, median=None, mad=None):\n",
    "      \"\"\"Robust Z-Score for better robustness against outliers in train set.\"\"\"\n",
    "      if median is None:\n",
    "        median = df_train[name].median()\n",
    "      if mad is None:\n",
    "        absolute_deviation = (df_train[name] - median).abs()\n",
    "        mad = absolute_deviation.median()\n",
    "      if mad == 0:\n",
    "        mad = 1\n",
    "      df[name] = (df_source[name] - median) / mad * 0.6745\n",
    "      bounds = max(df[name].max(), -df[name].min()) / 2\n",
    "      sampler = Gaussian(0., 1., bounds)\n",
    "      self.add(sampler)\n",
    "    def encode_text_dummy(self, df, df_source, name):\n",
    "      uniques = df_source[name].nunique()\n",
    "      if uniques == 1:\n",
    "        dummy_name = f\"{name}-{df_source[name].iloc[0]}\"\n",
    "        df[dummy_name] = 1.\n",
    "      elif uniques <= 2:\n",
    "        dummy_name = f\"is-{name}\"\n",
    "        dummies = pd.get_dummies(df_source[name], drop_first=True)\n",
    "        df[dummy_name] = dummies[list(dummies.columns)[0]]\n",
    "      else:  # No sparse when more than 1 class to ensure same distance between everyone\n",
    "        dummies = pd.get_dummies(df_source[name])\n",
    "        for x in dummies.columns:\n",
    "          dummy_name = f\"{name}-{x}\"\n",
    "          df[dummy_name] = dummies[x]\n",
    "      sampler = Categorical(uniques)\n",
    "      self.add(sampler)\n",
    "    def fit_transform(self, df_source, df_train, continuous_policy, discrete_cols=[]):\n",
    "      assert continuous_policy in ['robust', 'logscale', 'zscore']\n",
    "      cols = list(df_source.columns)\n",
    "      df = pd.DataFrame(index=df_source.index)\n",
    "      for col in cols:\n",
    "        if col == 'label':\n",
    "          continue\n",
    "        if col in discrete_cols:\n",
    "          self.encode_text_dummy(df, df_source, col)\n",
    "        elif continuous_policy == 'robust':\n",
    "          self.encode_robust_zscore(df, df_source, df_train, col)\n",
    "        elif continuous_policy == 'logscale':\n",
    "          self.encode_logscale(df, df_source, col)\n",
    "        elif continuous_policy == 'zscore':\n",
    "          self.encode_numeric_zscore(df, df_source, df_train, col)\n",
    "      df['label'] = df_source['label'].copy()\n",
    "      self.check_integrity(16, (df.shape[1]-1,))\n",
    "      return df\n",
    "    def sample(self, batch_size):\n",
    "      samples = [sampler.sample(batch_size) for sampler in self.samplers]\n",
    "      samples = np.concatenate(samples, axis=1)\n",
    "      return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5dde1-ca3e-4f79-aee4-b3b3de347f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Sampler(bounds = 5)\n",
    "continuous_policy = 'zscore'\n",
    "dt = sampler.fit_transform(df, normality, continuous_policy=continuous_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c2264d-c041-4367-bf5a-b5538dfd277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt[dt['label'] == 0.].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85859f-af1f-4011-a351-f52ec978d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt[dt['label'] == 1.].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc4115-e744-46d6-ad72-103748bb5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = pd.DataFrame(sampler.sample(10))\n",
    "adv.columns = dt.columns[:-1]\n",
    "adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d454a-8ec6-4b24-85df-d3b18cf021cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists, indexes = kdt.query(adv.to_numpy(), k=20, return_distance=True)\n",
    "pd.DataFrame(dists).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d578c312-66ed-44be-b4c4-99883041eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import scipy\n",
    "\n",
    "def compute_precision_recall(ytest, yanomalies, T):\n",
    "  pred_test = ytest >= T\n",
    "  pred_anomalies = yanomalies < T\n",
    "  tp = pred_test.sum()\n",
    "  tn = pred_anomalies.sum()\n",
    "  fp = len(yanomalies) - tn\n",
    "  fn = len(ytest) - tp\n",
    "  recall_in = tp / (tp + fn + 1e-8) * 100\n",
    "  recall_out = tn / (tn + fp + 1e-8) * 100\n",
    "  precision_in = tp / (tp + fp + 1e-8) * 100\n",
    "  precision_out = tn / (tn + fn + 1e-8) * 100\n",
    "  preds = pred_test, pred_anomalies\n",
    "  precision_recall = (recall_in, recall_out, precision_in, precision_out)\n",
    "  return preds, precision_recall, T\n",
    "\n",
    "def seek_equal_precision_recall(ytest, yanomalies):\n",
    "  \"\"\"Apply protocole of TQM and HRN to make precision=recall=F1.\"\"\"\n",
    "  a = min(ytest.min(), yanomalies.min())\n",
    "  b = min(ytest.max(), yanomalies.max())\n",
    "  def fun(T):\n",
    "    preds, precision_recall, T = compute_precision_recall(ytest, yanomalies, T)\n",
    "    recall_in, _, precision_in, _ = precision_recall\n",
    "    return recall_in - precision_in\n",
    "  T = scipy.optimize.bisect(fun, a, b)\n",
    "  return T\n",
    "\n",
    "def evaluate(epoch, model, xtest, anomalies, threshold=None):\n",
    "    test_size, anomalies_size = len(xtest), len(anomalies)\n",
    "    xx = np.concatenate([xtest, anomalies], axis=0)\n",
    "    _ = model(anomalies, training=True) # garbage in to apply bjorck.\n",
    "    yy = model.predict(xx, verbose=1, batch_size=2048).flatten()\n",
    "    ytest, yanomalies = np.split(yy, indices_or_sections=[test_size])\n",
    "    mean_in, std_in = ytest.mean(), ytest.std()\n",
    "    mean_out, std_out = yanomalies.mean(), yanomalies.std()\n",
    "    if threshold is None:\n",
    "       threshold = seek_equal_precision_recall(ytest, yanomalies)\n",
    "    preds, precision_recall, threshold = compute_precision_recall(ytest, yanomalies, threshold)\n",
    "    pred_test, pred_anomalies = preds\n",
    "    recall_in, recall_out, precision_in, precision_out = precision_recall\n",
    "    true_labels = np.concatenate([np.ones(test_size), np.zeros(anomalies_size)], axis=0)\n",
    "    roc_auc = roc_auc_score(true_labels, yy)\n",
    "    pred_yy = np.concatenate([pred_test, 1-pred_anomalies], axis=0)\n",
    "    f1 = f1_score(true_labels, pred_yy) * 100\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    percentiles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    trainstats = pd.DataFrame(ytest).describe(percentiles).transpose()\n",
    "    teststats = pd.DataFrame(yanomalies).describe(percentiles).transpose()\n",
    "    stats = pd.concat([trainstats,teststats], ignore_index=True).round(4)\n",
    "    stats.index = ['test','anomalies']\n",
    "    if (epoch+1)%5== 0:\n",
    "        print(stats)\n",
    "    margin_T = (threshold - config.margin) / config.margin * 100\n",
    "    print(f\"Mean-In={mean_in:.2f}±{std_in:.2f} Mean-Out={mean_out:.2f}±{std_out:.2f} T={threshold:.4f} TMarginError={margin_T:.2f}%\")\n",
    "    print(f\"False-Alarm={100-recall_in:.2f}% Sensivity-Anomalies={recall_out:.2f} Precision-Anomaly={precision_out:.2f}%\")\n",
    "    print(f\"[IMPORTANT] ROC_AUC={roc_auc:.2f} Precision={precision_in:.2f}% Recall={recall_in:.2f}% F1={f1:.2f}\")\n",
    "    if plot_wandb:\n",
    "      wandb.log({'roc_auc': roc_auc, 'recall':recall_in, 'precision':precision_in, 'f1':f1, 'T':threshold, 'TMarginError':margin_T})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a7fda-19f6-47de-8334-b700cd9107e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create positive examples dataset.\n",
    "p_dataset = build_ds_from_numpy(x_train, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b911ab30-ec8c-4f3c-bf47-c6b2366590a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer class.\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.0005)\n",
    "\n",
    "# Initialize the network.\n",
    "gen = tf.random.Generator.from_seed(4321)  # reproducible sampling.\n",
    "p_batch = next(iter(p_dataset))\n",
    "_ = model(p_batch, training=True)  # dummy forward to trigger initialization.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ebba1-1465-4d14-b332-9a512e810adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial distribution.\n",
    "def sampler_fn(gen, batch_size, input_shape):\n",
    "  del gen  # unused.\n",
    "  del input_shape  # unused.\n",
    "  return sampler.sample(batch_size)\n",
    "\n",
    "q_dataset = tfds_from_sampler(sampler_fn, gen, config.batch_size, p_batch.shape[1:])\n",
    "Q0 = next(iter(q_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1087c2-489d-40df-8055-d1b302333170",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = config.warmup_epochs\n",
    "for epoch in range(num_epochs):\n",
    "  train(model, opt, loss_fn, gen, p_dataset, q_dataset, epoch_length, maxiter=0, **train_kwargs)\n",
    "  evaluate(epoch, model, x_test, anomalies)\n",
    "plot_preds_ood(epoch, model, tf.constant(x_train), tf.constant(x_test), tf.constant(anomalies), plot_histogram=True, plot_wandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8384b0c2-dff0-4fba-8e4f-e64ccd89b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(config.epochs_per_plot):\n",
    "  train(model, opt, loss_fn, gen, p_dataset, q_dataset, epoch_length, maxiter=config.maxiter, **train_kwargs)\n",
    "  evaluate(epoch, model, x_test, anomalies)\n",
    "plot_preds_ood(epoch, model, tf.constant(x_train), tf.constant(x_test), tf.constant(anomalies), plot_histogram=True, plot_wandb=plot_wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f5677d-9447-40a9-b6cf-1c21b405b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(config.epochs_per_plot):\n",
    "  train(model, opt, loss_fn, gen, p_dataset, q_dataset, epoch_length, maxiter=config.maxiter, **train_kwargs)\n",
    "  evaluate(epoch, model, x_test, anomalies)\n",
    "plot_preds_ood(epoch, model, tf.constant(x_train), tf.constant(x_test), tf.constant(anomalies), plot_histogram=True, plot_wandb=plot_wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9417869d-0014-4be0-bc1e-6c53236035ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_wandb:\n",
    "  wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75cb657-4d73-4de7-8ada-2fe68f192840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d81943c-0a63-4d62-90f5-59cb01c487c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d555d-5a82-4499-8be9-75efba760865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e4d680-3741-4eac-8d59-687f9fe1bf96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d55e15-a474-40cb-9d72-7833c343a6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
